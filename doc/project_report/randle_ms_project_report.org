#+LATEX_CLASS: article
#+LaTeX_HEADER: \usepackage[a4paper, total={7in, 10in}]{geometry}
#+LaTeX_HEADER: \usepackage{minted}
#+LaTeX_HEADER: \usepackage[backend=biber, style=numeric]{biblatex}
#+LaTeX_HEADER: \addbibresource{randle_ms_project_report.bib}
#+LaTeX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{caption}
#+LaTeX_HEADER: \usepackage[parfill]{parskip}
#+LaTeX_HEADER: \captionsetup{font=footnotesize,labelfont={bf,footnotesize}}
#+LaTeX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \hypersetup{colorlinks, citecolor={blue!50!black}, linkcolor={blue!35!black}, urlcolor={blue!80!black}}
#+LaTeX_HEADER: \usepackage[section]{placeins}
* Abstract

* Introduction
** Background
UAF purchased the Clearpath Jackal and Husky as assets to aid in mining emergency operations. Pogo Mine expressed interest in 
unmanned exploration and mapping facilities and Clearpath provided these platforms as turn key solutions. Several graduate students worked alongside ACUASI to further develop the platforms for UAF specific use, and to integrate UAS (unmanned ariel systems).

The Jackal and Husky provided many untapped capabilities largely due to inconvenient user interface and out-of-date software. The platforms utilize Robot Operating System (ROS) for communication and control. ROS versions are linked to specific versions of linux distributions - this is problematic and becomes more problematic as time goes by; a computer with with that specific linux distribution must be on hand to utilize ROS visualization and command capabilities (outside of using the hard-linked driving controller). Without these features, the Clearpath robots are of little value; they cannot be driven out of visual range and their included sensors are unusable.

The platforms also include base station tripod WiFi extenders and long range WiFi antennas mounted on the robots in order to provide extended WiFi network range. In the original configuration, the clearpath robots combine with the base stations to create an adhoc WiFi network. The robots and base stations make use of Ubiquity Bullet M2 extended range routers; the firmware on several of these routers had been overwritten by OpenWRT firmware rendering them useless, and the remaining routers were configured incorrectly. Also The base station extenders were missing batteries and no longer operational.

Finally, the Husky was no longer operational. The included controller no longer worked, and both clearpath robots are out of warranty.

** Project Scope
Restore clearpath platforms to a working state. Upgrade the Jackal/Husky with the latest Clearpath linux distributions and ROS packages. Restore the base stations and setup/configure the Bullet M2 routers, along with the Jackal/Husky network cards using linux netplan. Document the process of restore, setup, and configuration.

Create a web app to provide visualization and control of the Jackal/Husky, with a user interface tuned for both smart phone and desktop through a web browser. Provide different end points for full control/visualization and visualization only to allow participants to view data in read only fashion - make endpoints available to anyone
connected to the Clearpath WiFi network. Allow driving the Husky/Jackal through the control endpoint without disabling control through the wireless controller. Show a two-dimensional map with the robot and obstacles localized on the map, and provide an interface for setting autonomous navigation goals. Provide a way to reset the map without requiring robot restart.

** Related Works
There are several web and smart phone apps which provide ROS interfaces, allowing remote robot visualization and control.
Foxglove
ROSControlCenter
ROSweb
ROSboard

* System Overview

Each Clearpath robot is configured to connect or establish a Wifi hostspt with SSID "clearpath". When the base stations are powered up, they connect and extend the clearpath network

* Robot Operating System (ROS)
From the front page of ros.org, ROS is "a set of software libraries and tools that help you build robot applications". There are tutorials and explanations on how every part of ROS works available here \autocite{rosmain}, but here we summarize the parts of ROS utilized in this project.

Just like an operating system provides a standard platform/environment for applications to run, ROS provides a standard platform/environment for robot applications. All ROS tools run on top of a linux distribution, and are invoked with CLI (command line interface) just like native GNU tools (such as a C compiler, or grep). That begs the question - why not just use linux executables? That is basically what ROS is - except that it standardizes and abstracts inter-process and inter-machine communication. It does this mainly through nodes, topics, and messages.

** Setup
If using a compatible linux distrobution (currently Ubuntu 20.04 is the latest supported version) ROS can be installed using apt package manager \autocite{rosinstall}. ROS Noetic was installed on the development machine using:
#+begin_src bash
  $ sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" > /etc/apt/sources.list.d/ros-latest.list'
  $ sudo apt install curl
  $ curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add -
  $ sudo apt update
#+end_src

Once installed, a bash setup script located at /opt/ros/kinetic/setup.bash must be sourced in order for ROS tools to be available on the command line. This is a pattern that ROS uses repeatedly; set environment variables and system values by sourcing bash scripts. This allows ROS to alter system settings and provide a convenient shell interface without invading or changing the system - the settings are dropped once the shell is terminated.

In order to fully utilize ROS and build packages, some dependencies are needed. The following was used to install these dependencies on the development machine:

#+begin_src bash
  $ sudo apt install python3 python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential
  $ sudo rosdep init
  $ rosdep update
#+end_src

An ROS system can include multiple machines, but exactly one of the machines must be designated as the ROS Master (without complicated multi-master setup). Most ROS utilities provide command line arguements to specify which machine is the master, but specifying it this way is tedious and error prone. Since an ROS system includes several executables, all of which need to know who is designated master, the environment variable ROS_MASTER_URI can be set to specify the master globablly. The uri is in the format http://HOSTNAME:PORT where hostname can be either the machine name or IP address and the port can be any available open port of choice - but the docs suggest a default of 11311. ROS_MASTER_URI defaults to http://localhost:11311, so leaving it unset sets the ROS master to self.

An ROS system is started by running roscore in a terminal (after sourcing the setup script) on the designated master machine. Since ROS_MASTER_URI defaults to localhost, it can be left as default for the master. All other machines must set ROS_MASTER_URI in the terminal to the machine running roscore before invoking any ROS commands - leaving it as localhost in this case will fail with "Unable to communicate with master" as roscore has not been run on the machine. If roscore is started on multiple machines (all leaving ROS_MASTER_URI as localhost), then each machine would be running its own ROS system and ROS executables would be unable to communicate with eachother.

The Jackal and Husky run separate ROS systems - on startup they both run roscore with ROS_MASTER_URI pointing to localhost on port 11311. It is theoretically possible to use the same ROS system for both robots, but would be difficult and provide little benefit (this would be called a multi-master setup). The command roscore starts an ROS master daemon, a parameter server, and a node (a process) called rosout for logging. The ROS master daemon is responsible for connecting ROS nodes (processes) to eachother on request. Once the connection is made - nodes engage in peer to peer communication. The parameter server provides a server for nodes to register static and dynamic parameters, and the rosout node provides logging.

To setup the development machine to communicate with either the Jackal or the Husky easily, we add some lines to ~/.bashrc which is sourced on terminal startup. For convenience, we also source the ROS setup script.

#+begin_src bash
  source /opt/ros/noetic/setup.bash
  export ROS_MASTER_URI=http://cpr-uaf01:11311
  #export ROS_MASTER_URI=http://cpr-uaf02-husky:11311
#+end_src

To choose which machine just comment out the appropriate line - commenting both will set the master to self (this is used for simulation as we will discuss later). Every terminal will now have ROS commands available.

** Nodes, Packages, and Launch Files
A node is a process which is started by running an executable file on disk (or forked from another process) just like any other system process. In order for an executable to qualify as a node, when its built it must link with the ROS library and register itself (with ROS master process) on startup. ROS nodes can be built using c++ or python; if using c++ the node must link with roscpp and if using python it must link with rospy. Once a node is installed (either through apt or by building from source) it can be started in a terminal by using "rosrun node_name".

The easist way to create a ROS node is by creating a package and placing the node source in the created package. Packages are the basic "project" unit in ROS - the most simple package would be a folder containing a file named package.xml and CMakeLists.txt. The xml file specifies package dependencies, name, author, and other such meta information. The CMakeLists.txt file is a file specifying how to build the project using CMake \autocite{cmake}. To build a node with c++, configure the CMakeLists.txt to point to the source code and build the project with catkin.

ROS ships with a tool called catkin for creating the boiler plate code needed for a package, and for building one or many packages at once. Catkin calls in to CMake, and so uses CMakeLists.txt files for building. To use catkin, create a catkin workspace and place each package under a subfolder in the workspace called src. A typical workspace would look like:
#+begin_src bash
  catkin_ws/
      build/ # Subfolders not listed - contains build artifacts
      devel/ # Subfolders not listed - contains the resulting executables and bash scripts after building
      src/
          CMakeLists.txt # symbolic link pointing to /opt/ros/noetic/share/catkin/cmake/toplevel.cmake
          package1/
              CMakeLists.txt
              package.xml
              ...
          ...
          packageN/
              CMakeLists.txt
              package.xml
              ...
#+end_src
The catkin workspace lives on the local machine building the packages only - this wouldn't be committed to source control - the packages would be. Often, when there are multiple interdependent packages, rather than placing each package in its own repository they are grouped in a single repo. In this case, all packages can be cloned directly into the catkin_ws/src folder. To create a workspace:
#+begin_src bash
  $ mkdir -p ~/catkin_ws/src && cd ~/catkin_ws
  $ catkin_make
#+end_src
where catkin_ws can be called anything.

Though nodes can individually be started, often multiple nodes need to be started simultaneously and work together as a group. ROS provides another command line tool, roslaunch, which takes a package name and launch file as parameters. Launch files are special config files which can be added to packages by placing the file/s in a launch subfolder:
#+begin_src bash
  package1/
      CMakeLists.txt
      package.xml
      launch/
          your_launch_file.launch
#+end_src
A launch file allows specifying nodes that should be started when the launch file is called with roslaunch. In the above example, the launch file would be loaded by calling:
#+begin_src bash
  $ roslaunch package1 your_launch_file.launch
#+end_src
As long as the launch file is in the launch subfolder of package1 it will be found.

*** Packages From Source
Most ROS packages are installed using the package manager (apt install ros-noetic-package-name), however some must be built from source. This could be a custom coded package, or a package that was never added to the apt repository.

To build an ROS package with catkin, the package must first be added to the catkin workspace. Without catkin, packages can be built directly with cmake but building with catkin provides setup bash files in the devel (and install if wanted) workspace subfolders. Just as sourcing the main ROS setup script adds ROS commands to the path, sourcing the setup script adds all built targets to the path so they are callable from ROS tools. Assuming a catkin workspace is setup as previously shown and the package is added to the workspace, to build simply use:
#+begin_src bash
  $ catkin_make
  $ catkin_make install # optional
#+end_src
Once the setup.bash script in the devel (or install) subfolder of the workspace is sourced, all ROS commands will work with any of the built packages as if they were installed with the package manager. It makes sense, then, to also add lines to .bashrc file to source any workspaces used for ROS package development.

** Topics, Messages, and Parameters
ROS nodes communicate with eachother through topics and messages. Messages are data schemas - similar to a struct in C or a table in SQL. The basic building block data types can be found in the package std_msgs - but custom messages can be composed by using any other message as members. For example, a couple of important messages in this project:

#+begin_src python
  ## geometry_msgs/Twist
  Vector3  linear
  Vector3  angular

  ## geometry_msgs/Vector3
  float64 x
  float64 y
  float64 z
#+end_src

The Twist message is used to convey driving velocity commands - the linear describes velocity along each axis while the angular describes velocity about each axis. The values don't have any direct relation to units - each robot chooses min/max values and correlates them to driving motor speeds.

Topics are named destinations for certain message types. By sending and receiving messages to/from topics rather than to/from nodes directly, nodes require no direct information about other nodes - they only require the string names and message types of topic of interest. As mentioned earlier, the rosmaster process is in charge of establishing connections between nodes who publish/subscribe to the same topic.

A topic is created by publishing a message to the topic - this can be done in c++ or python code within a node, or using the rostopic pub command. Once a message is published to a topic for the first time, the topic is linked to that message type and ROS logs errors if other message types are published to that topic. The rostopic list command can be used to get a complete list of the current topics:

#+begin_src bash
    $ rostopic list
  /rosout
  /rosout_agg
#+end_src

This message/topic system is the most fundemental thing that makes ROS useful. For example, a vendor can build a LIDAR device any way they want; to make it ROS compatible the vendor would write a node which publishes LaserScan messages to the scan topic. Usually there is a way to configure which topic the message would publish to, in case there are multiple LIDARs or scan is being used for something else. One way to provide a customization point is through parameters.

As part of roscore, ROS starts a parameter server. The server provides an API for nodes to register variables that are customizable and stores these variables and their values as a dictionary. Parameters can be set and retreived via c++ and python API, as well as through the command line tool rosparam. What paremeters do exactly is node dependent, but usually they provide a way to alter the node's behaviour at runtime. The navigation stack, for example, makes use of parameters to configure things like which navigation algorithm should be used, or how often should the pathfinding loop execute. Parameters can also be set for nodes using launch files, but only on node startup.

While nodes can use messages to communicate with eachother, messages tend to be used for active dynamic data while parameters are used for more static node configuration.

** Driving
Both the Jackal and Husky include wireless controllers which directly drive the robots. In each robot there is an ROS node called bluetooth_teleop/joy_node which reads the joystick device file at /dev/js and converts this to a Joy message:

#+begin_src python
    Header header   # timestamp in the header is the time the data is received from the joystick
    float32[] axes  # the axes measurements from a joystick
    int32[] buttons # the buttons measurements from a joystick
#+end_src

The message is posted to the topic bluetooth_teleop/joy, which another node called bluetooth_teleop/teleop_twist_joy subscribes to. This node translates the buttons and axis from the controller message to velocity drive commands and posts Twist messages containing these commands to the bluetooth_teleop/cmd_vel topic.

There is a node called /twist_mux which is responsible for multiplexing the velocity command messages from the controller nodes with velocity commands from other sources. For example, using an ROS tool called rviz \autocite{rviz}, it is possible to drive the Jackal/Husky by dragging drive arrows shown in *Figure [[fig:jackal_rviz]]* with the mouse.

#+caption: Jackal in rviz with driving controls
#+name:   fig:jackal_rviz
#+attr_latex: :width 5in
#+ATTR_HTML:  :width 70% :height auto
[[./images/jackal_rviz.png]]
\FloatBarrier

On dragging, RViz sends forward/back/left/right/rotate commands to the node twist_marker_server which translates these commands to Twist messages and publishes these messages to the twist_marker_server/cmd_vel topic. The twist_mux node subscribes to all cmd_vel topics and produces a single Twist message which is published to jackal_velocity_controller/cmd_vel. The motor control board (or gazebo if simulating) then directly controls the jackals drive motors based on this message.

#+caption: Node/topic layout - nodes in ovals topics and namespaces in rectangles
#+name:   fig:drive-topics
#+attr_latex: :width 7in
#+ATTR_HTML:  :width 100% :height auto
[[./images/rqt_graph.png]]
\FloatBarrier

The link from /twist_mux to /jackal_velocity_controller/cmd_vel is removed for clarity. The easiest way to drive the Jackal and Husky programatically is to publish Twist messages to one of the /cmd_vel topics. This can be done directly using rostopic pub:

#+begin_src bash
$ rostopic pub -r 10 /cmd_vel geometry_msgs/Twist  '{linear:  {x: 0.0, y: 0.0, z: 0.0}, angular: {x: 0.0,y: 0.0,z: 0.3}}'
#+end_src

This spins the Husky or Jackal around at 0.3 rads per second. Since the jackal can only drive forward/backward and rotate, y and z do nothing in linear portion of the message and x and y do nothing in the angular portion of the message. Linear x (+/-) is used to drive forward and backward, and to turn left/right angular x (+/-) is used.

** Transform Heirarchy
ROS represents 6 DOF geometric items' positions and orientations as transforms from parent to child coordinate frames. The parent/child relationship of transformations forms a heirarchy where each frame is a node in the world graph - this is identical to a scene graph in rendering libraries. A frame's position and orientation is always releative to it's parent frame, with the root frame having no parent.

Frames can be added to the heirarchy by publishing TFMessage to the /tf or /tf_static topic. The /tf_static topic can be used to publish transforms that don't change - all tf_static transforms are broadcast to subscribers only once when the nodes first subscribe. The format for the message is (all different message types shown in block for convenience):

#+begin_src python
  ## tf2_msgs/TFMessage
  geometry_msgs/TransformStamped[] transforms;

  ## geometry_msgs/TransformStamped
  Header header
  string child_frame_id # the frame id of the child frame
  geometry_msgs/Transform transform

  ## std_msgs/Header
  uint32 seq
  time stamp
  string frame_id

  ## geometry_msgs/Transform
  geometry_msgs/Vector3 translation # This is already shown above
  geometry_msgs/Quaternion rotation

  ## geometry_msgs/Quaternion
  float64 x
  float64 y
  float64 z
  float64 w
#+end_src

The frame_id in the header is the parent frame; the transform is giving the translation and rotation of child_frame_id in relation to frame_id. To work out the orientation and position of child frames in the root coordinate frame (or what some libraries would call "world coordinates"), we start at the root coordinate frame and build a four by four transformation matrix from the root frame orientation and position, which for the root frame is equal to the translation and rotation.

Using row major matrix layout, the transform matrix is created by first creating a rotation matrix from the quaternion orientation as shown in equation (1) \autocite{quatrot} (with x y z and w from the quaternion message above), and using the rotation matrix as the basis for the 4x4 matrix. The 3D position is then set as the last column in the 4x4 matrix, with 1 left as the last element in the column.

\begin{gather}
  Rot_{3x3} = 
  \begin{bmatrix}
    1-2y^{2}-2z^{2} & 2xy+2wz & 2xz-2wy\\
    2xy-2wz & 1-2x^{2}-2z^{2} & 2yz+2wx\\
    2xz+2wy & 2yz-2wx & 1-2x^{2}-2y^{2}
  \end{bmatrix}
\end{gather}

The following C/C++ code creates a 4x4 transformation matrix given the translation vector and rotation quaternion of a frame:

#+begin_src cpp
  struct quaternion
  {
      float x;
      float y;
      float z;
      float w;
  };

  struct vector3
  {
      float x;
      float y;
      float z;
  };

  struct matrix4
  {
      float data[4][4];
  };

  matrix4 create_transform(const quaternion &rotation, const vector3 &translation)
  {
    matrix4 ret {};

    // Build the rotation basis from the passed in rotation quaternion
    ret.data[0][0] = 1 - 2 * (rotation.y * rotation.y + rotation.z * rotation.z);
    ret.data[0][1] = 2 * (rotation.x * rotation.y - rotation.z * rotation.w);
    ret.data[0][2] = 2 * (rotation.x * rotation.z + rotation.y * rotation.w);

    ret.data[1][0] = 2 * (rotation.x * rotation.y + rotation.z * rotation.w);
    ret.data[1][1] = 1 - 2 * (rotation.x * rotation.x + rotation.z * rotation.z);
    ret.data[1][2] = 2 * (rotation.y * rotation.z - rotation.x * rotation.w);

    ret.data[2][0] = 2 * (rotation.x * rotation.z - rotation.y * rotation.w);
    ret.data[2][1] = 2 * (rotation.y * rotation.z + rotation.x * rotation.w);
    ret.data[2][2] = 1 - 2 * (rotation.x * rotation.x + rotation.y * rotation.y);

    // Add the translation vector as the last column
    ret.data[3][0] = translation.x;
    ret.data[3][1] = translation.y;
    ret.data[3][2] = translation.z;

    // Finally leave 1 in the last matrix position (m44) from the identity
    ret.data[3][3] = 1.0f;
    return ret;
  }
#+end_src

With the root node transform ready, we iterate over all child frames and build a transform as we did for the root frame. We then multiply the parent frame by the child frame as shown in equation 2 to get the child frame's world transform - which can be used to determine the child frame position and orientation in the same coordinates as the root frame.

\begin{gather}
  Transform_{world} = Transform_{parent} \times Transform_{child}
\end{gather}

Getting the position from the matrix is easy - its the first three elements of the last column. The derivation for getting the orientation as a quaternion from the matrix is a bit more complicated but is detailed in \autocite{tform_matrix}, along with the coordinate transformation process. The code below shows an example of grabbing it:

#+begin_src cpp
  quaternion orientation(const matrix3 &rotation)
  {
      quaternion ret; 
      float tr = rotation.data[0][0] + rotation.data[1][1] + rotation.data[2][2], s;

      if (tr > 0)
      {
          s = sqrt(tr + 1.0) * 2;
          ret.w = 0.25 * s;
          ret.x = (rotation.data[2][1] - rotation.data[1][2]) / s;
          ret.y = (rotation.data[0][2] - rotation.data[2][0]) / s;
          ret.z = (rotation.data[1][0] - rotation.data[0][1]) / s;
      }
      else if ((rotation.data[0][0] > rotation.data[1][1]) & (rotation.data[0][0] > rotation.data[2][2]))
      {
          s = sqrt(1.0 + rotation.data[0][0] - rotation.data[1][1] - rotation.data[2][2]) * 2;
          ret.w = (rotation.data[2][1] - rotation.data[1][2]) / s;
          ret.x = 0.25 * s;
          ret.y = (rotation.data[0][1] + rotation.data[1][0]) / s;
          ret.z = (rotation.data[0][2] + rotation.data[2][0]) / s;
      }
      else if (rotation.data[1][1] > rotation.data[2][2])
      {
          s = sqrt(1.0 + rotation.data[1][1] - rotation.data[0][0] - rotation.data[2][2]) * 2;
          ret.w = (rotation.data[0][2] - rotation.data[2][0]) / s;
          ret.x = (rotation.data[0][1] + rotation.data[1][0]) / s;
          ret.y = 0.25 * s;
          ret.z = (rotation.data[1][2] + rotation.data[2][1]) / s;
      }
      else
      {
          s = sqrt(1.0 + rotation.data[2][2] - rotation.data[0][0] - rotation.data[1][1]) * 2;
          ret.w = (rotation.data[1][0] - rotation.data[0][1]) / s;
          ret.x = (rotation.data[0][2] + rotation.data[2][0]) / s;
          ret.y = (rotation.data[1][2] + rotation.data[2][1]) / s;
          ret.z = 0.25 * s;
      }
      return ret;
  }
#+end_src

The transform heirarchy is crucial for mapping, localization, and navigation. Even without any simultaneous localization and mapping (SLAM) nodes enabled, the Jackal and Husky use odometry and the IMU information along with the transform heirarchy to produce an estimate as to where it is in the world. The root level frame in this case is called odom. The ekf localization node calculates the base_link rotation/translation (as relative to odom) and publishes the odom frame with the base_link as the child frame as shown in *Figure [[fig:ekf-localization]]*.

#+caption: ekf_localization node publishing to /tf based on multiple inputs
#+name:   fig:ekf-localization
#+attr_latex: :width 3in
#+ATTR_HTML:  :width 43% :height auto
[[./images/ekf_localization_node.png]]
\FloatBarrier

The complete jackal heirarchy without any SLAM nodes running is shown in *Figure [[fig:transform-heirarchy]]*. The husky is nearly identical - but has a few different leaf nodes for its sensors and geometry. Most of the transform frames are published by the /robot_state_publisher node. This node reads in a URDF file from the parameter server and publishes transforms to /tf based on the contents of the URDF file. URDF is a file format that lists robot joints and links; links correspond to frames, and joints list the parent/child relationships between frames - there are several tutorials here \autocite{urdf}.

#+caption: Transform heirarchy for the jackal
#+name:   fig:transform-heirarchy
#+attr_latex: :width 7in
#+ATTR_HTML:  :width 100% :height auto
[[./images/transform.png]]
\FloatBarrier

** SLAM GMapping
Simultaneous Localization and Mapping, or SLAM, refers to the process of reading in sensor data over time, using the changing sensor data to find landmarks and build a map, and then using the landmarks and map to localize the robot. One of the most widely used algorithms to do SLAM with two-dimensional planar LIDAR data is gmapping. The gmapping SLAM algorithm, as proposed in \autocite{gmapping}, is implemented on OpenSLAM \autocite{open_slam_gmapping}, and ported to ROS as a package \autocite{gmapping_package}. The package launches a node called slam_gmapping which subscribes to /front/scan and /tf, performs SLAM using the data on those topics, and publishes the resulting map as an OccupancyGrid to the /map topic as shown in *Figure [[fig:gmapping_graph]]*.

#+caption: GMapping package node and updated transform tree
#+name:   fig:gmapping_graph
#+attr_latex: :width 6in
#+ATTR_HTML:  :width 86% :height auto
[[./images/gmapping_graph.png]]
\FloatBarrier

A new transform frame called map is also published to the /tf topic as shown in *Figure [[fig:gmapping_graph]]*. The slam_gmapping node performs localization by calculating the odom translation/rotation (as relative to the map using gmapping SLAM algorithm) and publishing the map frame to /tf with the odom frame as its child. In the complete picture, ekf_localization updates the relative transform of base_link to odom by using the IMU/odometry, and slam_gmapping updates the relative transform of odom to map by using SLAM based on the LIDAR scan data.

The OccupancyGrid message format is:
#+begin_src python
  ## nav_msgs/OccupancyGrid
  std_msgs/Header header
  nav_msgs/MapMetaData info
  # The map data, in row-major order, starting with (0,0).
  # Occupancy probabilities are in the range [0,100].  Unknown is -1.
  int8[] data

  ## nav_msgs/MapMetaData
  time map_load_time
  # The map resolution [m/cell]
  float32 resolution
  # Map width [cells]
  uint32 width
  # Map height [cells]
  uint32 height
  # The origin of the map [m, m, rad].  This is the real-world pose of the
  # cell (0,0) in the map.
  geometry_msgs/Pose origin

  ## geometry_msgs/Pose
  Point position
  Quaternion orientation

  ## geometry_msgs/Point
  float64 x
  float64 y
  float64 z
#+end_src

This message type is used for costmaps in addition to maps. For maps, the values in data are either 0, 100, or -1; not the full range of [0-100]. With a map available, the navigation stack can run and create costmaps and use those costmaps to create drive paths.

** Navigation Stack
The navigation stack on ROS refers to the move_base node, the plugins the move_base node uses, and all the topics published by move_base. *Figure [[fig:nav_stack]]* shows an overview of the node/topic relationship \autocite{nav_stack}. The move_base node is configurable; the ovals inside of the move_base rectangle indicate parts of the node which run using plugins. This means the user can create a shared library and, with some configuration, move_base will use the shared library in place of the default behaviour.

#+caption: Navigation stack overview
#+name:   fig:nav_stack
#+attr_latex: :width 6in
#+ATTR_HTML:  :width 86% :height auto
[[./images/overview_tf_small.png]]
\FloatBarrier

The move_base node only issues velocity commands to the robot if there is an active navigation path, and there is only an active navigation path if the global planner has received a PoseStamped goal message on topics move_base/goal or move_base_simple/goal. The PoseStamped message is the Pose message with a Header added. The move_base node will start trying to drive the robot to the target pose once a PoseStamped message is received on either topic. Move_base publishes goal status' to the move_base/status topic in the form of an array of GoalStatus messages:

#+begin_src python
  ## actionlib_msgs/GoalStatusArray
  std_msgs/Header header
  actionlib_msgs/GoalStatus[] status_list

  ## actionlib_msgs/GoalStatus
  actionlib_msgs/GoalID goal_id
  string text    
  # 0 (PENDING) The goal has yet to be processed by the action server
  # 1 (ACTIVE) The goal is currently being processed by the action server
  # 2 (PREEMPTED) The goal received a cancel request after it started executing
  #   and has since completed its execution (Terminal State)
  # 3 (SUCCEEDED) The goal was achieved successfully by the action server (Terminal State)
  # 4 (ABORTED) The goal was aborted during execution by the action server due
  #   to some failure (Terminal State)
  # 5 (REJECTED) The goal was rejected by the action server without being processed,
  #   because the goal was unattainable or invalid (Terminal State)
  # 6 (PREEMPTING) The goal received a cancel request after it started executing
  #   and has not yet completed execution
  # 7 (RECALLING) The goal received a cancel request before it started executing,
  #   but the action server has not yet confirmed that the goal is canceled
  # 8 (RECALLED) The goal received a cancel request before it started executing
  #   and was successfully cancelled (Terminal State)
  # 9 (LOST) An action client can determine that a goal is LOST. This should not be
  #   sent over the wire by an action server
  uint8 status

  ## actionlib_msgs/GoalID
  time stamp
  string id
#+end_src

The Jackal and Husky move_base implementations only work on a single goal at a time, but if a goal is cancelled and another goal issued quickly, multiple goals can appear in the status array. To get the correct goal in all cases, the goal id associated with the original goal pose as in the PoseStamped message is needed. On receiving a goal, move_base will publish a MoveBaseActionGoal message to the topic /move_base/goal which gives the pose and id of the goal:

#+begin_src python
  ## move_base_msgs/MoveBaseActionGoal
  Header header
  actionlib_msgs/GoalID goal_id
  MoveBaseGoal goal

  ## move_base_msgs/MoveBaseGoal
  geometry_msgs/PoseStamped target_pose

  ## geometry_msgs/PoseStamped
  std_msgs/Header header
  geometry_msgs/Pose pose
#+end_src

Since the MoveBaseActionGoal message contains the goal id as generated by move_base for the pose published by the user to move_base_simple/goal, the user can store the goal id and use it to parse the goal status updates. The move_base node builds two costmaps, local and global, to aid in building a navigation path. The global costmap feeds the global planner, and the local costmap and global path as determined by the global planner feed the local planner. The local and global planners are specified as plugins which allow custom planner packages to be used. The default local planner package for the Jackal and Husky is base_local_planner, and the default global planner package is navfn.

All planner nodes are put in a namespace within the package. For example, the node provided by base_local_planner is base_local_planner/TrajectoryPlannerROS, the node provided by navfn is navfn/NavfnROS, and the node provided by global_planner is global_planner/GlobalPlanner.

*** Costmaps
Both the local and the global costmaps contain data about nearby obstacles - the difference between the costmaps is in the configuration. The global costmap is usually configured to use map as its global frame, be the same size as the OccupancyGrid published to the /map topic, and not move along with the robot. The local costmap uses odom as its global frame, is much smaller than the global costmap, and stays centered over the robot as the robot moves.

Move base publishes the local costmap to the move_base/local_costmap/costmap topic, and the global costmap to the move_base/global_costmap/costmap topic. The message type for both topics is OccupancyGrid, where the data member of the message contains probabilities of an obstacle being at that map location. Move base uses the laser scan, and if a map is being published to the /map topic the map, to find where obstacles are at and then inflates the obstacles by the robot's footprint. The resulting 8 bit probability values are divided in to ranges as shown in *Figure [[fig:costmap_spec]]* \autocite{costmap_spec}.

#+caption: Costmap 8 bit probability spec considering robot footprint
#+name:   fig:costmap_spec
#+attr_latex: :width 4in
#+ATTR_HTML:  :width 57% :height auto
[[./images/costmap_spec.png]]
\FloatBarrier

*** Local Planner
The local planner is responsible for issuing velocity commands to the robot given a higher level path from the robot to a goal. The base_local_planner package implements this functionality with two different algorithms; Trajectory Rollout \autocite{trajectory_rollout}, and Dynamic Window Approach \autocite{dwa}. Both algorithms do the following \autocite{local_planner}:

1. Sample robot velocity control space,
2. Simulate trajectory for each possible velocity command if applied for a short time
3. Score each trajectory considering obstacles, proximity to goal, proximity to global path, and speed
4. Send highest scoring velocity to robot

*** Global Planner
The global planner takes in the global costmap and a goal and produces a path from the robot's current location to the goal. To create a custom global planner in c++, for example, a class must be created inheriting from nav_core::BaseGlobalPlanner:

#+begin_src cpp
    class BaseGlobalPlanner{
    public:
      virtual bool makePlan(const geometry_msgs::PoseStamped& start, 
          const geometry_msgs::PoseStamped& goal, std::vector<geometry_msgs::PoseStamped>& plan) = 0;

      virtual void initialize(std::string name, costmap_2d::Costmap2DROS* costmap_ros) = 0;
  };
};  // namespace nav_core
#+end_src

The makePlan function, which must be implemented by every global planner (pure virtual), takes a starting pose and goal and must fill in plan which is a reference to a vector of poses. The worst planner, for example, could add the starting pose and goal pose to plan and return which would likely get the robot stuck. The package global_planner provides implementations of A* and Dijkstra's path planning algorithms (as well as a few others), which are selectable via parameters. Navfn only provides A*.

** Simulation
Simulating the Jackal and Husky is easy; Clearpath provides packages which launch and set up the ROS simulation tool, Gazebo \autocite{gazebo}, with everything needed to simulate the robots. The simulation is somewhat limited with sensor nodes and data, but it simulates laser scan data and driving which is enough for most development work on our app. To simulate the Jackal and Husky, first install the correct packages:

#+begin_src bash
$ sudo apt-get install ros-noetic-jackal-simulator ros-noetic-jackal-desktop # Replace jackal with husky for the husky
#+end_src

And to start the simulation with the laser scan enabled:

#+begin_src bash
$ roslaunch jackal_gazebo jackal_world.launch config:=front_laser # Again, replace jackal with husky for the husky
#+end_src

The default jackal_world.launch and husky_playground.launch files create the worlds shown in *Figure [[fig:jackal_husky_sim]]*. The default worlds can be edited; its possible to create new geometry or load in models created by other Gazebo users. The model database contains tons of content, including complete office buildings! For this project, we stuck with the default worlds as the scope of the project didn't require anything else.

#+caption: Jackal and Husky default simulation worlds
#+name:   fig:jackal_husky_sim
#+attr_latex: :width 6in
#+ATTR_HTML:  :width 85% :height auto
[[./images/jackal_husky_sim.png]]
\FloatBarrier

It is worth mentioning, before launching Gazebo, the machine running the simulation must be configured as the ROS master with roscore running.

* Clearpath Robot Configuration
Text

** Upgrade ROS from ROS Indigo to ROS Noetic
Text

** Configure sensor nodes to run at startup
Text

** Configure Bumblebee Camera (Jackal Only)
Text

** Configure network using Netplan
Text

** Configure Bluetooth PS4 Controllers (upgraded for Jackal)
Text

** Fix Husky E-Stop malfunction
Text

* Wifi Network
Text

** Network Overview
Text

** Restore Base Stations
Text

** Router Firmware Upgrade
Text

** Router Configuration
Text

** Custom DNS Servers
Text

* UAF Clearpath Control Web Application
The web application provides an interface to ROS control and navigation data for the Jackal and Husky through a web browser. The basic driving/mapping/autonomous navigation interface does not require any ROS knowledge. There is an interface for getting/setting ROS parameters for users who need more control.

On Jackal and Husky startup, an http server starts which serves the application to any clients who connect. The frontend app served to the connecting clients is written in c/c++ and uses Emscripten \autocite{emscripten} to cross compile to WebAssembly. Urho3D \autocite{urho3d} is used for rendering, user interface, and user input processing; Urho3D is also cross compiled to WebAssembly using Emscripten. Node.js \autocite{nodejs} is used for the backend to create the http server and uses rosnodejs \autocite{rosnodejs} to interface with ROS.

The application also supports targeting Desktop, but only Mac/Linux as it uses POSIX sockets. The nodejs backend starts a second POSIX server on a different port to allow desktop connections. Targeting desktop is useful during the development process. From this point on we refer to the nodejs backend application as the _server_ app and the frontend application that is served to clients as the _client_ app.

** Setup and Build
Building the client app requires CMake along with tools sourced from build-essentials linux package. All builds require downloading and building Urho3D from source, and the web build requires downloading and installing the Emscripten library. The server node application only requires node/npm.

*** Emscripten
Emscripten version 2.0.8 is used for this project, as that is the latest supported version for building Urho3d. To download and install Emscripten v2.0.8:
#+begin_src bash
  $ git clone https://github.com/emscripten-core/emsdk.git
  $ cd emsdk
  $ ./emsdk install 2.0.8
#+end_src

The easiest way to use the Emscripten compiler is to use the provided toolchain file with CMake. A toolchain file is a special file which sets up CMake for cross compiling. After issuing the install command, the Emscripten toolchain file is located at emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake. To use it with CMake, pass it in when invoking CMake:
#+begin_src bash
  $ cmake -DCMAKE_TOOLCHAIN_FILE=<path_to_emsdk>/emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake
#+end_src

*** Urho3D
We use a forked copy of Urho3D based on tag 1.9.0. It is forked because the repository has been archived, and some source code modifications are necessary for bugfixes specific to our app. The source code can be downloaded with:

#+begin_src bash
  $ git clone https://github.com/dprandle/urho3d.git
  $ cd urho3d && git checkout 1.9.0
#+end_src

We use two different build configurations; linux (aka desktop) and emscripten. The linux desktop build has debug symbols enabled, but the emscripten build does not as the target runs in a web browser. For both builds the samples are turned off as they signigicantly increase the build time. We export compile commands for the linux build (since it uses g++) for easy browsing of the urho source code in editors such as vscode or emacs which make use of clangd. The options are listed in *Table [[table:build_config]]*. The directory where Emscripten was cloned is shown as <ems_dir>.

#+caption: Build configuration options for Urho3D - blank means the value is not passed to CMake
#+name:   table:build_config
#+attr_latex: :width 7in :font \scriptsize
| *CMake Variable*              | *Linux Build* | *Emscripten Build*                   |
|-------------------------------+---------------+--------------------------------------|
| URHO_SAMPLES                  | OFF           | OFF                                  |
| CMAKE_EXPORT_COMPILECOMMANDS  | TRUE          | FALSE                                |
| CMAKE_BUILD_TYPE              | Debug         | Release                              |
| CMAKE_TOOLCHAIN_FILE          |               | <ems_dir>/upstream/emscripten/cmake/ |
|                               |               | Modules/Platform/Emscripten.cmake    |
| WEB                           |               | TRUE                                 |
| EMSCRIPTEN_ROOT_PATH          |               | <ems_dir>/upstream/emscripten        |
| EMSCRIPTEN_SYSROOT            |               | <ems_dir>/upstream/emscripten/system |
| EMSCRIPTEN_ALLOW_MEMRY_GROWTH |               | TRUE                                 |

The build_urho3d.sh script located in the client app source root configures and builds Urho3D for both configuration. The location of the Urho3D root source directory must be passed in as the -u arguement, and the Emscripten root source directory as the -e arguement. If -u isn't present, the script will try to use ../urho3d as the Urho3D directory, and if -e isn't passed in it will try to use ../emsdk as the Emscripten directory:
#+begin_src bash
  $ ./build_urho3d -u <path/to/urho3d> -e <path/to/emscripten> # Build using paths specified
  $ ./build_urho3d -u <path/to/urho3d> # Build with path specified for Urho and ../emsdk for Emscripten
  $ ./build_urho3d -e <path/to/emscripten> # Build with path specified for Emscripten and ../urho3d for Urho  
#+end_src

The configure and build process can take a while - grab a coffee. The urho build artifacts are created in <urho_dir>/build/linux for the linux build, and in <urho_dir>/build/emscripten for the emscripten build.

*** Client Application
With Emscripten and Urho3D setup, the client app is ready to build. The CMakeLists.txt files (one in the root folder, one in the src folder) make use of cmake functions defined by Urho3D, and so they need to know, for each build configuration, where the build artifacts for Urho3D are located. As with Urho3D, the linux build includes debug symbols and exports compile commands while the emscripten build does not.

The build options are shown in *Table [[table:client_build_config]]*. The Urho3D root directory is shown as <urho_dir> and the Emscripten root directory is shown as <ems_dir>. The value for URHO3D_HOME is shown assuming the build script was used to configure/build Urho3D. If it wasn't, the values should be replaced with the build location for each configuration.

#+caption: Build configuration options for client application - blank means the value is not passed to CMake
#+name:   table:client_build_config
#+attr_latex: :width 7in :font \scriptsize
| *CMake Variable*               | *Linux Build*          | *Emscripten Build*                   |
|--------------------------------+------------------------+--------------------------------------|
| CMAKE_EXPORT_COMPILE_COMMANDS  | TRUE                   | FALSE                                |
| CMAKE_BUILD_TYPE               | Debug                  | Release                              |
| URHO3D_SRC                     | <urho_dir>             | <urho_dir>                           |
| URHO3D_HOME                    | <urho_dir>/build/linux | <urho_dir>/build/emscripten          |
| CMAKE_TOOLCHAIN_FILE           |                        | <ems_dir>/upstream/emscripten/cmake/ |
|                                |                        | Modules/Platform/Emscripten.cmake    |
| WEB                            |                        | TRUE                                 |
| EMSCRIPTEN                     |                        | TRUE                                 |
| EMSCRIPTEN_ALLOW_MEMORY_GROWTH |                        | TRUE                                 |

The build_app.sh script located in the client app source root configures and builds the client application. As with the build_urho3d.sh script, the Urho3D root directory is passed in as the -u arguement, and the Emscripten root directory with -e arguement (with the same defaults). Leaving out -e completely for this script will build for linux desktop, while including it at all will build using emscripten:

#+begin_src bash
  $ ./build_app.sh -u <path/to/urho3d> -e <path/to/emscripten> # Build with Emscripten using paths specified
  $ ./build_app.sh -u <path/to/urho3d> -e # Build with Emscripten using path specified for Urho and ../emsdk for Emscripten
  $ ./build_app.sh -e # Build with Emscripten using ../urho3d for Urho and ../emsdk for Emscripten
  $ ./build_app.sh -u <path/to/urho3d> # Build for linux desktop using path specified for Urho
  $ ./build_app.sh # Build for linux desktop using ../urho3d for Urho
#+end_src

Building the client app for linux produces a normal linux binary. Building the client app with Emscripten produces four files:
 - uaf_clearpath_ctrl.wasm: WebAssembly - our c/c++ code gets compiled in to this binary blob which is runnable within the browser sandbox
 - uaf_clearpath_ctrl.data: Contains all resources which are read from files (images, shaders, config files, models, etc)
 - uaf_clearpath_ctrl.js: Contains javascript functions generated by emscripten which call in to our WebAssembly
 - uaf_clearpath_ctrl.html: This is a shell html file which serves as the entry point - it imports and calls functions in uaf_clearpath_ctrl.js to load uaf_clearpath_ctrl.data and uaf_clearpath_ctrl.wasm

It's possible to setup a local server to serve the html file directly from the build path, but we are setup to deploy these files to the uaf_clearpath_ctrl_server folder located locally (for development and debugging) or on the Jackal/Husky. The deploy_app.sh script can be used to deploy the files. Pass in the local folder path with -l, -j to try and deploy to Jackal, and -h to try and deploy to th Husky. The -l, if no arguement is used, defaults to ../uaf_clearpath_ctrl_server, the -j defaults to the Jackal hostname cpr-uaf01, and the -h defaults to the Husky hostname cpr-uaf02-husky.

#+begin_src bash
  $ ./deploy_app.sh -l -j -h # attempt to deploy the resulting build files to ../uaf_clearpath_ctrl_server, administrator@cpr-uaf01:~/uaf_clearpath_ctrl_server, and administrator@cpr-uaf02-husky:~/uaf_clearpath_ctrl_server
  $ ./deploy_app.sh -l <custom/server/path> # Only deploy files locally to <custom/server/path>
  $ ./deploy_app.sh # Do nothing
  $ ./deploy_app.sh -j # Deploy to jackal only using default hostname cpr-uaf01 
#+end_src

The files are actually deployed to the src/emscripten subfolder within the uaf_clearpath_ctrl_server. If browsers have the app open when the files are deployed, the page will need to be refreshed for the changes to take effect. On both the Jackal and the Husky, the uaf_clearpath_ctrl_server folder is located in the home directory.

*** Server Application
NodeJS and NPM are needed to install the server. To install all required packages, use npm install. The following clones the server, installs the dependencies, and then starts it:
#+begin_src bash
  $ git clone https://github.com/dprandle/uaf_clearpath_ctrl_server.git
  $ cd uaf_clearpath_ctrl_server
  $ npm install
  $ npm start
#+end_src

The client side app, as previously mentioned, is located in src/emscripten. The javascript run by nodejs, which starts the servers, is located in src/index.js. All code for the server is in this one file, which npm is configured to run on npm start. Since the Jackal and Husky run the server as a service using systemctrl, deploying new versions of index.js either requires either rebooting or restarting the systemctrl service.

** Feature/Interface Overview
There are two different interfaces depending on which URL is used to load the app. The control interface includes everything, while the observer interface is a subset which doesn't provide any mechanism for driving or setting ROS parameters. *Figure [[fig:control_observer_phone]]* shows the control and observer interface.

#+caption: Control interface on left and observer interface on right on phone
#+name: fig:control_observer_phone
#+attr_latex: :width 6in
#+ATTR_HTML:  :width 85% :height auto
[[./images/control_observer_phone.png]]
\FloatBarrier

An overview of the control interface is shown in *Figure [[fig:control_interface_overview]]*. This is viewing the simulated jackal in Google Chrome on Linux desktop.

#+caption: Desktop web browser view of full interface
#+name: fig:control_interface_overview
#+attr_latex: :width 6in
#+ATTR_HTML:  :width 85% :height auto
[[./images/control_interface_overview.png]]
\FloatBarrier

*** Feature List
The following is a list of implemented features:

**** Robot
- Show live robot position/orientation
- Show the robot model rendered using the ROS transform tree

**** Joystick
- Allow driving the robot in any direction at speeds in proportion to the joystick offset from centerghh    hwr

**** View Panel
- Show/hide the live laser scan
- Show/hide map as it's generated
- Show/hide the live camera feed if connected to Jackal
- Show/hide the local/global costmaps as they're generated/updated
- Show/hide the local/global navigation paths as they are generated/updated

**** Toolbar
- Enable/disable automatic camera following the robot
- Add/remove multiple navigation waypoints for autonomous navigation
- Reset the navigation stack and clear all maps
- Get and Set ROS parameters
- Send all other connected clients a message
- Measure paths

**** Output Panel
- Show results and errors on setting ROS parameters
- Show received messages from other clients
- Show misc notifications from the server

**** Camera Controls
- Pan left/right/up/down
- Zoom in and out

**** Misc Stats
- Show number of clients currently connected to robot
- Show instantaneous and average payload bandwidth (does not include packet overhead) for all connected clients

** Startup
The entry point, like most c applications, is the function main:
#+begin_src cpp
int main(int argc, char **argv)
{
    auto args = urho::ParseArguments(argc, argv);
    auto urho_ctxt = new urho::Context;
    jackal_control_ctxt jctrl{};
    jctrl.urho_ctxt = urho_ctxt;

    if (!jctrl_init(&jctrl, args))
        return 0;

    jctrl_exec(&jctrl);
    jctrl_term(&jctrl);
}
#+end_src

First we parse the command line arguements - this is for desktop builds. On the desktop build, the arguement -ip can be used to specify the ip address and -port for the port of the robot to connect to. If left out, these default to local host (127.0.0.1) and port 4000. For desktops with lower DPI/resolution, the -ui_scale option can be specified to scale all UI elements. The jackal_control_ctxt structure contains top level data structures required to run the app.

#+begin_src cpp
  struct jackal_control_ctxt
  {
      urho::Context* urho_ctxt {};
      urho::Engine * urho_engine {};

      ui_info ui_inf;

      joystick_panel js_panel;
      map_panel mpanel;
      input_data inp;

      net_connection conn;

      ss_router router;
  };
#+end_src

The app is entirely setup and initialized in the jctrl_init function, which takes the command line arguements and the jackal_control_ctxt structure as parameters. Each module is initialized within this initialization function:

#+begin_src cpp
  bool jctrl_init(jackal_control_ctxt *ctxt, const urho::StringVector & args)
  {
      ctxt->urho_engine = new urho::Engine(ctxt->urho_ctxt);

      int port {4000};
      urho::String ip{"127.0.0.1"};
      parse_command_line_args(&port, &ip, &ctxt->ui_inf.dev_pixel_ratio_inv, args);

      if (!init_urho_engine(ctxt->urho_engine, ctxt->ui_inf.dev_pixel_ratio_inv))
          return false;

      log_init(ctxt->urho_ctxt);

      ilog("Initializing jackal control");

      log_set_level(urho::LOG_DEBUG);
      setup_ui_info(&ctxt->ui_inf, ctxt->urho_ctxt);
      setup_main_renderer(ctxt);

      input_init(&ctxt->inp.dispatch, ctxt->urho_ctxt);
      ctxt->inp.map.name = "global";
      ctxt->inp.dispatch.inv_pixel_ratio = ctxt->ui_inf.dev_pixel_ratio_inv;
      ctxt->inp.dispatch.context_stack.push_back(&ctxt->inp.map);

      net_connect(&ctxt->conn, ip.CString(), port);
      joystick_panel_init(&ctxt->js_panel, ctxt->ui_inf, &ctxt->conn);

      ctxt->mpanel.ctxt = ctxt;
      map_panel_init(&ctxt->mpanel, ctxt->ui_inf, &ctxt->conn, &ctxt->inp);

      ss_connect(&ctxt->router, ctxt->js_panel.in_use, [ctxt](bool in_use) {
          ctxt->mpanel.js_enabled = in_use;
      });    

      ilog("Device pixel ratio inverse: %f", ctxt->ui_inf.dev_pixel_ratio_inv);
      return true;
  }
#+end_src

The init_urho_function sets up the Urho3D window parameters, log file for desktop (no logging for web builds), and calls the Urho3D engine initialization function. This sets up all of the default Urho3D "sub-systems" which are obtained by calling GetSubsystem<SystemType> method of the Urho3D context.

The function jctrl_exec runs the main application loop which differs for Desktop vs web builds:

#+begin_src cpp
  intern void jctrl_run_frame(jackal_control_ctxt *ctxt)
  {
      net_rx(&ctxt->conn);
      ctxt->urho_engine->RunFrame();
  }

  #if defined(__EMSCRIPTEN__)
  intern void run_frame_proxy(void *data)
  {
      auto ctxt = (jackal_control_ctxt *)data;
      jctrl_run_frame(ctxt);
  }
  #endif

  void jctrl_exec(jackal_control_ctxt *ctxt)
  {
  #if !defined(__EMSCRIPTEN__)
      while (!ctxt->urho_engine->IsExiting()) {
          jctrl_run_frame(ctxt);
      }
  #else
      emscripten_set_main_loop_arg(run_frame_proxy, ctxt, -1, true);
  #endif
  }
#+end_src

The =__EMSCRIPTEN__= preprocessor symbol is defined in web builds using Emscripten. The function emscripten_set_main_loop_arg is an Emscripten library function which takes a callback function pointer and a user void pointer as the first two parameters; the user pointer is passed as the arguement to the callback function. The web browser controls the application loop, the passed in callback function is called every frame. In either desktop or web builds, jctrl_run_frame is called every frame which reads in any incoming network data with net_rx and calls RunFrame on the urho::Engine which gathers all input, updates all objects in urho scene, updates all user interface items, and renders the results.

The user interface is setup differently depending on whether or not the app is started as a controller or observer.

** Observer and Controller
The web build differentiates between a controller and observer by parsing the url from the web browser address bar - the desktop build always runs as a controller. Javascript is required to get the full browser url path. Emscripten provides a way to write javascript functions within c source code by using the EM_JS(...) construct. The EM_JS macro takes the C function return type as the first parameter, the C function name as the second parameter, the C function parameters as the third parameter, and the javascript function definition as the fourth parameter:

#+begin_src cpp
EM_JS(char*, get_browser_url_path, (), {
    const path = window.location.pathname;
    const length = lengthBytesUTF8(path) + 1;
    const str = _malloc(length);
    stringToUTF8(path, str, length);
    console.log(`Should be returning  ${length} bytes for ${path}`);
    return str;
});  
#+end_src

All code within the curly brackets is javascript, and has access to the DOM, browser javascript functions (ie window.location.pathname), and Emscripten javascript functions (lengthBytesUTF8, _malloc, stringToUTF8). The get_browser_url_path function is called like a normal C function to get the full browser path:

#+begin_src cpp
    char *url_path = get_browser_url_path();
    conn->can_control = strncmp(url_path, "/control", 8) == 0;
    ilog("URL PATH: %s", url_path);
    free(url);  
#+end_src

This code checks for the presence of /control within the url_path and sets conn->can_control if it is there. The conn variable refers to a net_connection struct which is shown in detail in the Networking section. The conn->can_control flag is used to setup the user interface, disabling any control/driving elements if false.

This is not meant to be a secure mechanism. This is purely to avoid chaos when giving demonstrations at public events while still allowing spectators to view mapping data in real time on their phones.

** Scene
The urho::Scene object is the Urho3D interface used to add items to the transform heirarchy \autocite{urho_scene}. Each item in the scene is represented as an urho::Node which can have a single parent node and multiple child nodes. All items in the scene are children of the scene node; the urho::Scene class inherits from urho::Node and serves as the root node for the transform heirarchy. The scene also provides an interface for retreiving nodes by name, id, and tag.

Urho3D uses a flavor of Entity-Component-System (ECS) \autocite{ecs} to organize node data and behavior. Rather than using inheritance and subclassing urho::Node to implement different behaviors per node type, behaviors are implemented by creating urho::Component types and attaching components to nodes to give that node a certain behavior. Rather than placing the behavior implementation code within the nodes or components, the behavior code is placed in a "sub-system" which operates on all nodes containing the component type of interest; the component is basically plain old data. What ECS calls "entities", Urho3D calls "nodes"; both act as meta-data for objects within the library. Unlike with ECS, in Urho3D every node has a transform - the transform is not outsourced to a transform component. Also, in general, Urho3D wraps all components in classes with accessor functions, some of which alter the component state on calling.

The Urho3D renderer subsystem operates on all node's which have any component inheriting from urho::Drawable. Most nodes in this application have the urho::StaticModel component attached which references a model file to render. The scan data is shown using a node with the urho::BillboardSet component attached.

*** Urho3D Rendering
Rendering is done by the urho::Renderer by gathering all nodes with urho::Drawable components and rendering each item according to it's drawable component type and options \autocite{urho_renderer}.

The urho::Renderer is setup in setup_main_renderer which is called from jctrl_init:
#+begin_src cpp
intern void setup_main_renderer(jackal_control_ctxt *ctxt)
{
    auto graphics = ctxt->urho_ctxt->GetSubsystem<urho::Graphics>();
    graphics->SetWindowTitle("Clearpath Control");

    auto cache = ctxt->urho_ctxt->GetSubsystem<urho::ResourceCache>();
    auto rpath = cache->GetResource<urho::XMLFile>("RenderPaths/clear_only.xml");
    urho::Renderer *rnd = ctxt->urho_ctxt->GetSubsystem<urho::Renderer>();
    urho::Viewport *vp = new urho::Viewport(ctxt->urho_ctxt, nullptr, nullptr);
    vp->SetRenderPath(rpath);
    rnd->SetViewport(0, vp);

    auto zn = rnd->GetDefaultZone();
    zn->SetFogColor({0.0, 0.0, 0.0, 1.0});
}  
#+end_src

We setup the renderer by creating a viewport, attaching a render path to the viewport, and setting the renderer's viewport to the created viewport. A render path is an object which declares a number of rendering commands, where each command describes some operation to apply to a render target \autocite{urho_render_path}. For example, a scenepass command specifies rendering all nodes in the scene that reference a technique (through a referenced material, which is referenced by a static model component) that contains the specified "pass" to the render target specified by "output". A clear command clears the render target specified by "output" to the specified "color". The default render target is used, which is the screen backbuffer, when output isn't specified. All commands are executed in the order listed in the XML file. The render path used for the app is simple:

#+begin_src xml
  
#+end_src

*** Transform Tree
Text

*** Jackal/Husky Models
Text

*** Camera
Text

** UI
Text
*** Creating Icons with Inkscape
Text

*** UI File and Anchoring
Text

*** Input
Text

*** Scaling for different pixel ratios
Text

*** Toolbar
Text

*** Console
Text

*** View Toggle Panel
Text

** Networking
Text
*** Packing and Unpacking Data
Text

*** Packet Structure and Header
Text

*** Client/Server Routing with Sockets and WebSockets
Text

*** Sending/Receiving Data
Text

*** Bandwidth
Text

** Server ROS Interface with rosnodejs
Text

** Joystick Driving
Text

** Map Building
Text

** Autonomous Waypoint Navigation
Text

** Getting and Setting ROS Parameters
:PROPERTIES:
:ID:       a893a693-0450-4f2b-a804-09e9b6167990
:END:
Text

** Live Camera Feed (Jackal Only)
Text

** Misc
Text

*** Connection tracking
Text

*** Measuring paths
Text

*** Broadcast messages
Text

* Conclusion
Text

** Project Summary
Text

** Lessons Learned
Text

** Future Work
Text

** Final Remarks
Text

\newpage

\printbibliography
